---
title: "HW6"
format: html
editor: visual
---

## Stats 506 Homework 6

Margaret Miles

## Problem #1: Rcpp

In the notes, we defined a C_mean function. Using this as a template, implement a C_moment function that returns the kth central moment. Generate a vector of moderate length and show that you are able to replicate the results of e1071::moment.

Notes & Hints: Be cognizant of your scaling factor. Be sure to look at the arguments of e1071::moment.

```{r}
library(Rcpp)
#' C_moment function
#' 
#' Takes in a vector and returns the kth central moment
#' central moment is defined as m = E[(x-mu)^k] = 
#' m = 1/n*sum(xi - mean)^k
#' 
#' @params v = vector 
#' @params k = number in central moment
#' @returns moment = kth central moment
cppFunction("
            double C_moment(NumericVector v, int k){
            double sum = 0;
            double n = v.length();
            double mean = 0;
            
            for (int j = 0; j < v.length(); ++j){
                mean += v[j];
            }
            double m = mean / v.length();
            
            for (int i = 0; i < v.length(); ++i){
                double x = v[i] - m;
                sum += std::pow(x, k);
            }
            return(sum/n);
            }")

```

Generate a vector of moderate length and show that you are able to replicate the results of e1071::moment.

```{r}
library(e1071)

set.seed(123)
v <- rnorm(1000)

# test several moments
C_moment(v, 2)       # variance (not sample variance!)
moment(v, order=2, center=TRUE)

C_moment(v, 3)       # third central moment
moment(v, order=3, center=TRUE)

C_moment(v, 4)       # fourth central moment
moment(v, order=4, center=TRUE)
```

## Problem #2:Expanding on waldCI

a.  Write a class bootstrapWaldCI that produces a CI using bootstrap, similar to waldCI. It should inherit from waldCI using either the version from the solutions or your version - in either case, include the relevant code in this homework by using source() on a file containing the waldCI code.

The booststrapWaldCI constructor should take in a data set and a function that returns a scalar. E.g.

makeBootstrapCI(function(x) mean(x\$myvar), data = mydata, reps = 100)

Add an optional level argument and an optional compute argument that accepts either “serial” (using no parallel processing), “parallel” (using either forks or sockets from parallel).


The constructor should carry out the bootstrap using the requested compute approach. Most functions can be inherited from waldCI, but add the additional function rebootstrap that performs a new bootstrap. rebootstrap should only take in one argument of a bootstrapWaldCI object.

Note: Be careful with inheritance and S4:

The \@.Data slot only applies if the child class contains an S3 class; if it contains S4, it simply adds the childs slots alongside the parent slots. Slot names should not be re-used unless you’re explicltly overwriting existing slots.

```{r}

```

b.  Show your code works by executing the following:

ci1 \<- makeBootstrapCI(function(x) mean(x\$y), ggplot2::diamonds, reps = 1000) ci1 rebootstrap(ci1)

```{r}
ci1 <- makeBootstrapCI(function(x) mean(x$y),
                       ggplot2::diamonds,
                       reps = 1000)
ci1
rebootstrap(ci1)
```

Compare and comment on the performance of the two compute methods.

```{r}

```

c.  Write a function called dispCoef that takes in data (based upon mtcars; it must take in a generic data for the bootstrap) and fits the model: mpg \~ cyl + disp + wt. It should return the coefficient associated with disp. 

```{r}

```

Execute the following:

ci2 \<- makeBootstrapCI(dispCoef, mtcars, reps = 1000) ci2 rebootstrap(ci2)

```{r}
ci2 <- makeBootstrapCI(dispCoef,
                       mtcars,
                       reps = 1000)
ci2
rebootstrap(ci2)
```

Compare and comment on the performance of the two compute methods.

```{r}

```

## Problem #3: Large data

Generate artificial data by running the code in this script. Do not include this script in your submitted PDF; either use source() or save/load to get the data into R.

```{r}
# Load data-generating script
source("Problem3data.R")

# Assuming the script creates an object called 'bigdat' or similar
str(df)
```

a.  Fit one model per country. Fit a mixed effects logistic regression model, predicting course completion based upon prior GPA, number of forum posts, number of quiz attempts, and a random effect for device type. Standardize the predictors within each country. Generate some sort of visualization of the estimated coefficients for number of forum posts in each country.

```{r}
library(dplyr)
library(lme4)
library(purrr)
library(tidyr)
library(ggplot2)

# fit one model per country
data_list <- split(df, df$country)

# fit mixed effects logistic regression model
# predicting course completion based upon prior GPA, number of forum posts, number of quiz attempts, and a random effect for device type.
model_country <- function(dat) {
  # standardize each variable
  dat <- dat %>%
    mutate(
      #prior gpa
      std_gpa = scale(prior_gpa),
      #number of forum posts
      std_posts = scale(forum_posts),
      #number of quiz attempts
      std_attempt = scale(quiz_attempts)
    )
  
  # logistic regression with device time random effect
  glmer(
    completed_course ~ std_gpa + std_posts + std_attempt + (1 | device_type),
    data = dat,
    family = binomial(link = "logit")
  )
}

model1 <- model_country(data_list[[1]])
model2 <- model_country(data_list[[2]])
model3 <- model_country(data_list[[3]])
model4 <- model_country(data_list[[4]])
model5 <- model_country(data_list[[5]])
model6 <- model_country(data_list[[6]])

#plot estimated coefficients for number of forum posts in each country

# extract std_posts coefficient and std error
models <- list(model1, model2, model3, model4, model5, model6)
posts_coef <- data.frame(
  country = c("Germany","India","Lithuania","Nigeria","Other",
              "US"),
  coef = numeric(6),
  se = numeric(6)
)

for(i in 1:6){
  posts_coef$coef[i] <- fixef(models[[i]])["std_posts"]     
  posts_coef$se[i] <- sqrt(diag(vcov(models[[i]])))["std_posts"]
}
posts_coef

library(ggplot2)

ggplot(posts_coef, aes(x = country, y = coef)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = coef - 2*se, ymax = coef + 2*se), width = 0.2) +
  theme_minimal() +
  labs(
    title = "Effect of Forum Posts on Course Completion by Country",
    y = "Log-odds coefficient (standardized)",
    x = "Country"
  )
```

Report the running time (from system.time) for each of the 6 models.

```{r}
# fit models with run times
run1 <- system.time({
  model1 <- model_country(data_list[[1]])
})
run2 <- system.time({
  model2 <- model_country(data_list[[2]])
})
run3 <- system.time({
  model3 <- model_country(data_list[[3]])
})
run4 <- system.time({
  model4 <- model_country(data_list[[4]])
})
run5 <- system.time({
  model5 <- model_country(data_list[[5]])
})
run6 <- system.time({
  model6 <- model_country(data_list[[6]])
})
print(as.character(data_list[[1]]$country[1]))
run1
print(as.character(data_list[[2]]$country[1]))
run2
print(as.character(data_list[[3]]$country[1]))
run3
print(as.character(data_list[[4]]$country[1]))
run4
print(as.character(data_list[[5]]$country[1]))
run5
print(as.character(data_list[[6]]$country[1]))
run6

```

b.  Devise an approach that minimizes the running time of your script. Report the running time of your entire script (running models and estimating coefficients; no need for a new plot). Do not use a different package for the models. Show that the results match those from part a).

```{r}
library(dplyr)
library(lme4)
library(purrr)

# Split data by country
data_list <- split(df, df$country)

fit_country <- function(dat) {
  dat <- dat %>%
    mutate(
      std_gpa = scale(prior_gpa),
      std_posts = scale(forum_posts),
      std_attempt = scale(quiz_attempts)
    )
  
  glmer(
    completed_course ~ std_gpa + std_posts + std_attempt + (1 | device_type),
    data = dat,
    family = binomial(link = "logit")
  )
}

time2 <- system.time({
  # Fit all 6 models in one line using map
  models <- map(data_list, fit_country)
  
  # Extract std_posts coefficient and SE
  coef_df <- map_df(
    names(models),
    ~ tibble(
        country = .x,
        std_posts_coef = fixef(models[[.x]])["std_posts"],
        std_posts_se   = sqrt(diag(vcov(models[[.x]])))["std_posts"]
    )
  )
  coef_df
})

# time of faster version
print("Time of faster version: ")
time2
```

## Problem #4: **data.table**

Repeat problem set 4, question 2, using data.table.

You can make the same or different decision as you did last time. You can also use or ignore the decisions I made in the solutions. As before, there is no “correct” answer (including those in the problem set 4 solutions).

Use the tidyverse for this problem. In particular, use piping and dplyr as much as you are able. Note: Use of any deprecated functions will result in a point loss.

Use the “ATP Matches” data from 2019 available at https://raw.githubusercontent.com/JeffSackmann/tennis_atp/refs/heads/master/atp_matches_2019.csv. This data tracks all Tennis matches. This data does not have documentation, so you’ll have to explore the data yourself to figure out it’s structure. Use it to answer the following questions. Your answers should show both the output from R that allows you to answer it, as well as a written answer.

```{r}
tennis_matches <- read.csv("atp_matches_2019.csv")
library(skimr)
skim(tennis_matches)

library(data.table)
# Convert to data.table
setDT(tennis_matches)

```

a.  How many tournaments took place in 2019?

```{r}
# Convert date and extract year
tennis_matches[, tourney_date := as.character(tourney_date)]
tennis_matches[, year := as.numeric(substr(tourney_date, 1, 4))]

table(tennis_matches[,year])


# there are a lot of davis cups, combine them into one
tourney_2019_davis <- tennis_matches[year == 2019 & grepl("Davis", tourney_name, ignore.case = TRUE)]
davis_tourneys <- uniqueN(tourney_2019_davis$tourney_id)  # n_distinct equivalent

# now tournaments took place in 2019
tourn_2019 <- tennis_matches[year == 2019]
tot_tourneys <- uniqueN(tourn_2019$tourney_id)

# remove all but one davis tourney ID 
total_2019 <- tot_tourneys - davis_tourneys + 1

print(total_2019)

```

**Based on my R code, there were 66 tourneys in 2019.**

b.  Did any player win more than one tournament? If so, how many players won more than one tournament, and how many tournaments did the most winning player(s) win?

```{r}
# pull final round
finals <- tennis_matches[round == "F"]

# Count number of tournaments won per player
winner_counts <- finals[, .(tournaments_won = uniqueN(tourney_id)), by = winner_name]

# number of players that won 1< tournaments
multi_winners <- nrow(winner_counts[tournaments_won > 1])

# most winning player
top_winners <- winner_counts[tournaments_won == max(winner_counts$tournaments_won)]

```

**Assuming that every tourney has only one winner which is the winner of round "F" for final, and all tourneys have a distinct ID, there were 41 winners of tournaments, there were 12 winners with more than 1 win, and Dominic Thiem and Novak Djokovic won the most with 5.**

c.  Is there any evidence that winners have more aces than losers? (If you address this with a hypothesis test, do not use base R functionality - continue to remain in the Tidyverse.)

```{r}
# pull win aces 
win_aces <- tennis_matches[, .(total_aces = sum(w_ace, na.rm = TRUE)),
                           by = winner_name]
win_aces[, role := "Winner"]
lose_aces <- tennis_matches[, .(total_aces = sum(l_ace, na.rm = TRUE)),
                           by = loser_name]
lose_aces[, role := "Loser"]

# Rename player columns to match
setnames(win_aces, "winner_name", "player")
setnames(lose_aces, "loser_name", "player")


# Combine winners and losers
aces_combined <- rbindlist(list(win_aces, lose_aces))

aces_combined

# hypothesis test
t.test(total_aces ~ role, data = aces_combined)

```

**The mean number of aces for winners was higher than for losers. The two-sample t-test gives a p-value of p = 8.07e-6, suggesting that there is significant evidence that winners serve more aces than losers.**

d.  Identify the player(s) with the highest win-rate. (Note that this is NOT asking for the highest number of wins.) Restrict to players with at least 5 matches.

```{r}
# find total wins for each person
total_wins <- tennis_matches[, .(wins = .N), by = winner_name]

# total losses
total_losses <- tennis_matches[, .(losses = .N), by = loser_name]


# Aggregate wins losses to find matches per player
setnames(total_wins, "winner_name", "player")
setnames(total_losses, "loser_name", "player")
player_stats <- merge(total_wins, total_losses, by = "player", all = TRUE)
player_stats[, matches_played := wins + losses]

# now find rate
player_stats[, win_rate := wins / matches_played]

# Restrict to players with at least 5 matches
player_stats_5 <- player_stats[matches_played >= 5]

# find players with highest win rates
top_players <- player_stats_5[win_rate == max(win_rate)]

top_players
```
